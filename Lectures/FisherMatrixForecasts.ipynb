{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astronomy 8824 - Numerical and Statistical Methods in Astrophysics\n",
    "\n",
    "## Statistical Methods Topic IV. Fisher Matrix Forecasts and Linear Models\n",
    "\n",
    "These notes are for the course Astronomy 8824: Numerical and Statistical Methods in Astrophysics. It is based on notes from David Weinberg with modifications and additions by Paul Martini.\n",
    "David's original notes are available from his website: http://www.astronomy.ohio-state.edu/~dhw/A8824/index.html\n",
    "\n",
    "#### Background reading: \n",
    "- Statistics, Data Mining, and Machine Learning in Astronomy, $\\S4.2$ \n",
    "- Numerical Recipes, Chapter 15\n",
    "- Gould (2003), arXiv:astro-ph/0310577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "# matplotlib settings \n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('lines', linewidth=2)\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)   # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTex macros hidden here -- \n",
    "$\\newcommand{\\expect}[1]{{\\left\\langle #1 \\right\\rangle}}$\n",
    "$\\newcommand{\\intinf}{\\int_{-\\infty}^{\\infty}}$\n",
    "$\\newcommand{\\xbar}{\\overline{x}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\like}{{\\cal L}}$\n",
    "$\\newcommand{\\llike}{{\\rm ln}{\\cal L}}$\n",
    "$\\newcommand{\\xhat}{\\hat{x}}$\n",
    "$\\newcommand{\\yhat}{\\hat{y}}$\n",
    "$\\newcommand{\\xhati}{\\hat{x}_i}$\n",
    "$\\newcommand{\\yhati}{\\hat{y}_i}$\n",
    "$\\newcommand{\\sigxi}{\\sigma_{x,i}}$\n",
    "$\\newcommand{\\sigyi}{\\sigma_{y,i}}$\n",
    "$\\newcommand{\\cij}{C_{ij}}$\n",
    "$\\newcommand{\\cinvij}{C^{-1}_{ij}}$\n",
    "$\\newcommand{\\cinvkl}{C^{-1}_{kl}}$\n",
    "$\\newcommand{\\cinvmn}{C^{-1}_{mn}}$\n",
    "$\\newcommand{\\valpha}{\\vec \\alpha}$\n",
    "$\\newcommand{\\vth}{\\vec \\theta}$\n",
    "$\\newcommand{\\ymod}{y_{\\rm mod}}$\n",
    "$\\newcommand{\\dy}{\\Delta y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Fisher information matrix, or simply the Fisher matrix, describes the amount of information that some observables $D$ contain about the parameters ${\\mathbf \\theta}$ of the model distribution for $D$, that is $p(D | \\theta)$. \n",
    "\n",
    "In some cases, the probability density function $p(D | \\theta)$ may depend strongly on $\\theta$ such that one may need relatively little data derive $\\theta$. One example is the use of parallax measurements to determine the distance to a nearby star cluster. In this case, $\\theta$ is the distance of the cluster, and one would need relatively few measurements to determine $\\theta$ reasonably well. In other words, there is a lot of information about the parameters in the observables. \n",
    "\n",
    "In other cases, the probability density function $p(D | \\theta)$ may depend only weakly on the model parameters. An example is the galaxy luminosity function. The galaxy luminosity function is typically described by a power-law, an exponential cut-off at high luminosity, and a space density. A small number of measurements, especially if confined to a small luminosity range, would not provide much of a constraint. In this case there is relatively little information about parameters in the observables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Parameter Warmup\n",
    "\n",
    "Suppose we have an observable $y_1$ that we can predict given some model parameter $\\theta_1$, and that we measure $y_1$ with some observational error $\\sigma(y_1)$.\n",
    "\n",
    "Our best estimate of $\\theta_1$ is the value that gives the observed value of $y_1$.\n",
    "\n",
    "In the neighborhood of this best fit value $\\hat{\\theta}_1$, a linear Taylor expansion implies\n",
    "$$\n",
    "y_1(\\theta_1) = y_1(\\hat{\\theta}_1)+\n",
    "  \\left({dy_1 \\over d\\theta_1}\\right)(\\theta_1-\\hat{\\theta}_1).\n",
    "$$\n",
    "\n",
    "Simple \"chain rule\" error propagation then tells us that the error on $\\theta_1$ is\n",
    "$$\n",
    "\\sigma(\\theta_1) = \\left(dy_1 \\over d\\theta_1\\right)^{-1} \\sigma(y_1).\n",
    "$$\n",
    "\n",
    "Often we are interested in the fractional error\n",
    "$$\n",
    "{\\sigma(\\theta_1)\\over\\theta_1} = \\sigma(\\ln\\theta_1) =\n",
    "  \\left({d\\ln y_1 \\over d\\ln\\theta_1}\\right)^{-1}\\sigma(\\ln y_1).\n",
    "$$\n",
    "\n",
    "For example, if $y_1 \\propto \\theta_1^3$, $\\ln y_1 = 3 \\ln \\theta_1$ and \n",
    "$$\n",
    "\\left( \\frac{d\\ln y_1}{d\\ln \\theta_1} \\right) = 3\n",
    "$$\n",
    "so therefore\n",
    "$$\n",
    "\\frac{\\sigma(\\theta_1)}{\\theta_1} = \\sigma(\\ln \\theta_1) = \\frac{1}{3} \\sigma(\\ln y_1) = \\frac{\\sigma(y_1)}{3 y_1} \n",
    "$$\n",
    "and the fractional error on $\\theta_1$ is only 1/3 the fractional error on $y_1$.\n",
    "\n",
    "These results break down if the linear Taylor expansion becomes inaccurate over the observationally allowed range of $y_1$.\n",
    "\n",
    "In general, the error on a parameter depends on the error on the observable and on the sensitivity of that observable to that parameter. A more sensitive observable gives greater leverage on the parameter.\n",
    "\n",
    "A good takeaway is that errors on parameters depend on \n",
    "1. errors on observables\n",
    "2. sensitivity of the observables to the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Matrix Error Forecasting\n",
    "\n",
    "Suppose we are considering some future experiment rather than data we have in hand.  If we can \n",
    "1. Predict what the measurement errors on the data will be and\n",
    "2. We know how the data depend on model parameters\n",
    "Then we can forecast how accurately we will be able to constrain parameters. \n",
    "\n",
    "Examples: \n",
    "1. Given $\\sigma(mag)$, how many supernovae do we need to observe in order to obtain a 1\\% error on $\\Omega_\\Lambda$?\n",
    "2. If you want to measure the slope of the $M-\\sigma$ relation with some uncertainty, but can only observe two galaxies, what $\\sigma_*$ values should they have? \n",
    "\n",
    "If we have a parameter vector $\\vth$, the Fisher information matrix is defined by\n",
    "$$\n",
    "F_{ij} = - \\left\\langle{\\partial^2\\ln L \\over \\partial\\theta_i\\partial\\theta_j}\n",
    "                \\right\\rangle.\n",
    "$$\n",
    "The Fisher matrix (or Fisher Information Matrix) is thus the expected value of the curvature/Hessian matrix.\n",
    "\n",
    "A Hessian matrix is a square matrix of second-order partial derivatives that describes the local curvature of a function of many variables. It also contains the coefficients of the quadratic terms in a local Taylor expansion of a function.\n",
    "\n",
    "To the extent that the likelihood is well described by a quadratic Taylor expansion, the expected error on parameter $\\theta_i$ is\n",
    "$$\n",
    "\\sigma_i \\equiv \\sigma(\\theta_{i}) = (F^{-1}_{ii})^{1/2}\n",
    "$$\n",
    "if all of the parameters are being estimated from the data set and\n",
    "$$\n",
    "\\sigma_i \\equiv \\sigma(\\theta_{i}) = (F_{ii})^{-1/2}\n",
    "$$\n",
    "if all parameters other than $\\theta_i$ are known.\n",
    "\n",
    "Under more general conditions, the error of any unbiased estimator must be greater than or equal to these values, a result known as the *Cram\\'er-Rao Bound*. The Cram\\'er-Rao Bound is the lower bound on the variance of estimators of a parameter, and states that the variance of any estimator must be at least as high as the inverse of the Fisher  matrix. This is a statement of the *best* one can do. It is always possible to do worse!\n",
    "\n",
    "In _Fisher matrix forecasting_, we assume a fiducial model and properties of a data set to predict the Fisher matrix and thereby forecast the errors that will be obtained on model parameters.\n",
    "\n",
    "There is a pretty good high-level discussion of this in section 2 of Tegmark, Taylor, & Heavens (1997, ApJ, 480, 22)\n",
    "and a valuable but dense presentation in Gould (2003).\n",
    "\n",
    "Note that a Fisher matrix forecast only gives you accurate error forecasts *if* the 2nd-order expansion of the likelihood is accurate.\n",
    "\n",
    "If you're worried this might not be true, then you can use MCMC instead, with your anticipated measurement errors and setting the data equal to the values expected for your fiducial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter sensitivity and observational errors\n",
    "\n",
    "We can decompose a Fisher matrix into a matrix product:\n",
    "\n",
    "$$\n",
    "F_{ij} = - \\left\\langle{\\partial^2\\ln L \\over \\partial\\theta_i\\partial\\theta_j}\n",
    "                \\right\\rangle\n",
    "       = -\\left\\langle {\\partial \\dy_k \\over \\partial \\theta_i}\\cdot\n",
    "                     {\\partial^2 \\ln L \\over \\partial \\dy_k\\partial \\dy_l} \\cdot\n",
    "\t\t       {\\partial \\dy_l \\over \\partial \\theta_j}\\right\\rangle,\n",
    "$$\n",
    "\n",
    "where $\\dy_k = \\ymod(x_k)-y_k$ is the difference between the model prediction for data point $k$ and the observed value, and we are using the Einstein summation convention. Effectively the partial derivatives of $\\Delta y$ with respect to the parameters describe the sensitivity of the observables to the parameters, and the inner second derivative of the likelihood with respect to $\\Delta y$ describes the errors on the observables.\n",
    "\n",
    "Because the data values do not depend on the model parameters (they are just observed),\n",
    "$$\n",
    "{\\partial \\dy_k \\over \\partial \\theta_i} =\n",
    "{\\partial \\ymod(x_k) \\over \\partial \\theta_i}~.\n",
    "$$\n",
    "\n",
    "As we will show below, _if the errors on the observables are Gaussian and independent of the model parameters,_ then\n",
    "$$\n",
    "- {\\partial^2\\ln L \\over \\partial \\dy_k\\partial \\dy_l}  = \\cinvkl,\n",
    "$$\n",
    "the inverse covariance matrix.\n",
    "\n",
    "Thus, the Fisher matrix has an \"outer\" piece $\\partial \\vec{y}_{\\rm mod}/\\partial{\\vec{\\theta}}$ that represents\n",
    "the sensitivity of the observables to the parameters and an \"inner\" piece that represents the errors on the observables themselves.\n",
    "\n",
    "If we consider the 1-parameter, 1-observable case, $C^{-1} = 1/\\sigma^2_y$ we get\n",
    "$$\n",
    "F_{11} = {d\\ymod \\over d\\theta} \\cdot {1\\over \\sigma_y^2} \\cdot\n",
    "         {d\\ymod \\over d\\theta},\n",
    "$$\n",
    "Recall that $\\sigma_y$ are the uncertainties on the data $y$ and $\\sigma(\\theta)$ is on parameter $\\theta$. This result implies\n",
    "$$\n",
    "\\sigma(\\theta_1) = (F_{11})^{-1/2}\n",
    "$$\n",
    "$$\n",
    "\\sigma^2(\\theta) = 1/F_{11} = \\sigma_y^2 \\cdot\n",
    "  \\left({d\\ymod\\over d\\theta}\\right)^{-2},\n",
    "$$\n",
    "in agreement with our earlier chain rule result.\n",
    "\n",
    "For a Fisher matrix forecast of parameter errors, we compute the\n",
    "parameter sensitivity from our model, and we take the expected\n",
    "values of the observable errors (and their covariances).\n",
    "\n",
    "As far as I know, $\\partial{\\vec{y}_{\\rm mod}}/\\partial{\\vec{\\theta}}$ doesn't\n",
    "have a special name, but we can think of it as an \"influence matrix\"\n",
    "or \"sensitivity matrix.\"\n",
    "\n",
    "While computing the Fisher matrix requires assumptions about the\n",
    "data set, the sensitivity matrix requires only knowledge of the\n",
    "model, and it can be an interesting quantity to compute even if\n",
    "one doesn't have a specific data set in mind.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher matrix for Gaussian Likelihoods \n",
    "\n",
    "Suppose we have a Gaussian likelihood function for $N$ data points\n",
    "$$\n",
    "L = \\frac{1}{ (2 \\pi)^{N/2} \\sqrt{ {\\rm det} {\\bf C}} } \\exp \\left( -\\frac{1}{2} \\Delta y_m \\cinvmn \\Delta y_n \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\ln L = {N \\over 2}\\ln(2\\pi) + {1\\over 2}\\ln[{\\rm det}({\\bf C})]\n",
    "  + {1 \\over 2}\\dy_m\\cinvmn\\dy_n.\n",
    "$$\n",
    "(My reason for changing $kl$ indices to $mn$ indices will become evident shortly.)\n",
    "\n",
    "For the case of a diagonal covariance matrix (no covariance), $C_{mn}=\\sigma_m^2\\delta_{mn}$, this expression becomes\n",
    "$$\n",
    "-\\ln L = {N \\over 2}\\ln(2\\pi) + {1\\over 2}\\sum \\ln\\sigma_m^2 +\n",
    "  {1 \\over 2}\\sum {\\dy_m^2 \\over \\sigma_m^2},\n",
    "$$\n",
    "but we will consider the full covariant case.\n",
    "\n",
    "Assume that we can ignore any dependence of the covariance matrix on the model parameters.  _This is a non-trivial assumption that will not always hold._  \n",
    "\n",
    "For example, in cosmological applications we  sometimes have \"cosmic variance\" errors that depend on the amplitude\n",
    "of matter or galaxy clustering, and the expected size of these errors depends on the cosmological parameters.  \n",
    "\n",
    "However, if we have a data set that provides tight constraints on parameters, then the allowed model dependence of the covariance matrix usually cannot be large.\n",
    "\n",
    "If we do make this assumption, then the derivative of ${\\rm det}{\\bf C}$ with respect to parameters vanishes, and $\\cinvmn$ is independent of $\\dy_k$ and $\\dy_l$, allowing us to rearrange the\n",
    "\"inner\" piece of the Fisher matrix:\n",
    "$$\\eqalign{\n",
    "{1 \\over 2} {\\partial^2(\\Delta y_m\\cinvmn\\dy_n) \\over \\partial\\Delta y_k\\partial\\Delta y_l}\n",
    "&=\n",
    "{1 \\over 2} \\sum_{mn} \\cinvmn {\\partial^2(\\Delta y_m\\Delta y_n) \\over \n",
    "                               \\partial\\Delta y_k \\partial\\Delta y_l} \\cr\n",
    "&=\n",
    "{1 \\over 2} \\sum_{mn} \\cinvmn {\\partial \\over \\partial\\Delta y_k}\n",
    "   \\left({\\partial(\\Delta y_m\\dy_n) \\over \\Delta y_l}\\right) \\cr\n",
    "&=\n",
    "{1 \\over 2} \\sum_{mn} \\cinvmn {\\partial \\over \\partial\\dy_k}\n",
    "   \\left(\\dy_m {\\partial \\dy_n \\over \\partial \\dy_l} + \n",
    "         \\dy_n {\\partial \\dy_m \\over \\partial \\dy_l}\\right) \\cr\n",
    "&=\n",
    "{1 \\over 2} \\sum_{mn} \\cinvmn {\\partial \\over \\partial\\dy_k}\n",
    "   \\left(\\dy_m \\delta_{nl} + \\dy_n \\delta_{ml}\\right) \\cr\n",
    "&=\n",
    "{1 \\over 2} \\sum_{mn} \\cinvmn (\\delta_{km}\\delta_{nl} + \\delta_{kn}\\delta_{ml})\n",
    "  \\cr\n",
    "&= \n",
    "\\cinvkl~.\n",
    "}\n",
    "$$\n",
    "\n",
    "On the right-hand sides I have written out sums explicitly for clarity and interchanged sums and derivatives.  \n",
    "\n",
    "This derivation is a bit mathematically loose, but I think it is correct.\n",
    "\n",
    "Including the \"outer\" piece, the Fisher matrix is\n",
    "$$\n",
    "F_{ij} = \n",
    " {\\partial\\dy_k\\over \\partial\\theta_i}\\cinvkl\n",
    " {\\partial\\dy_l\\over\\partial\\theta_j} = \n",
    " {\\partial y_{\\rm mod}(x_k)\\over \\partial\\theta_i}\\cinvkl\n",
    " {\\partial y_{\\rm mod}(x_l)\\over\\partial\\theta_j}.\n",
    "$$\n",
    "\n",
    "Though notationally different, I think this is equivalent to equation (15) of Tegmark et al. (1997), except that the\n",
    "term ${\\bf A}_i{\\bf A}_j$ in that equation has vanished because we have assumed that the dependence of $C_{ij}$ on the parameters can be neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight Line Model\n",
    "\n",
    "Now consider a model $\\ymod(x) = \\theta_1 + \\theta_2 x$ where $\\Delta y_k = y_{mod}(x_k) - y_k$ and so $\\Delta y_k = \\theta_1 + \\theta_2 x_k - y_k$. \n",
    "\n",
    "The derivatives are\n",
    "$$\n",
    "{\\partial\\dy_k \\over \\partial\\theta_1} = 1, \\qquad\n",
    "{\\partial\\dy_k \\over \\partial\\theta_2} = x_k,\n",
    "$$\n",
    "making the $2\\times 2$ Fisher matrix\n",
    "$$\n",
    "F_{ij} = \\pmatrix{ \\sum\\cinvkl & \\sum \\cinvkl x_k \\cr\n",
    "                   \\sum\\cinvkl x_k & \\sum\\cinvkl x_k x_l}.\n",
    "$$\n",
    "\n",
    "This matrix can be inverted recalling that for \n",
    "$$\n",
    "A = \\pmatrix{ a & b \\cr c & d}, \\qquad\n",
    "A^{-1} = {1\\over ad-bc}\\pmatrix{d & -b \\cr -c & a}.\n",
    "$$\n",
    "\n",
    "The errors on the intercept and slope are, respectively, $(F_{11}^{-1})^{1/2}$ and $(F_{22}^{-1})^{1/2}$.\n",
    "\n",
    "For a diagonal covariance matrix $C_{kl} = \\delta_{kl}\\sigma_k^2$,\n",
    "$$\n",
    "F_{ij} = \\pmatrix{ \\sum\\sigma_k^{-2} & \\sum x_k\\sigma_k^{-2} \\cr\n",
    "                   \\sum x_k\\sigma_k^{-2} & \\sum x_k^2\\sigma_k^{-2}}\n",
    "       = {N \\over \\sigma^2} \\pmatrix{ 1 & \\langle x \\rangle \\cr\n",
    "                                      \\langle x \\rangle & \\langle x^2 \\rangle},\n",
    "$$\n",
    "where the second equality is for homoscedastic errors $\\sigma_k = \\sigma$. Note the summation over $ij$ is a summation over parameters, and the summation over $kl$ is over the data. \n",
    "\n",
    "Inverting the last case yields\n",
    "$$\n",
    "F_{ij}^{-1} = {\\sigma^2 \\over N}(\\langle x^2\\rangle - \\langle x \\rangle^2)^{-1}\n",
    "  \\pmatrix{ \\langle x^2 \\rangle & -\\langle x \\rangle \\cr\n",
    "            -\\langle x \\rangle & 1}.\n",
    "$$\n",
    "\n",
    "This is the forecast for the covariance matrix. \n",
    "\n",
    "This expression is the same as the top equation on p. 5 of Gould (2003). Here $F_{ij}^{-1}$ refers to the forecast covariance matrix of the parameter errors, and $\\langle x\\rangle$ and $\\langle x^2 \\rangle$ refer to expected properties of the data set. In Gould (2003), the $\\langle ... \\rangle$ averages are over the actual data points obtained, and the result is the actual covariance matrix of the parameter errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\chi^2$-minimization for a general linear model\n",
    "\n",
    "My discussion here follows that of Gould (2003) but with different notation.\n",
    "\n",
    "Our analysis of the straight-line model can be generalized to the fit of a model that is *linear* in the parameters $\\theta_i$,\n",
    "$$\n",
    "\\ymod(x) \\equiv \\sum_{i=1}^n \\theta_i f_i(x),\n",
    "$$\n",
    "where the $f_i(x)$ are specified functions.\n",
    "\n",
    "Note that the $f_i(x)$ do not need to be linear, e.g., we could have\n",
    "$$\n",
    "\\ymod(x) = \\theta_1 + \\theta_2 x + \\theta_3 x^2 +\\theta_4 \\sin(2\\pi x).\n",
    "$$\n",
    "\n",
    "Again defining $\\dy_k \\equiv y_k-\\ymod(x_k)$, we now have\n",
    "$$\n",
    "{\\partial \\dy_k \\over \\partial \\theta_i} = f_i(x_k).\n",
    "$$\n",
    "as $y_{mod}$ is linear in the $\\theta_i$. Therefore, for a Gaussian likelihood function,\n",
    "$$\n",
    "F_{ij} = f_i(x_k) \\cinvkl f_j(x_l) = {\\partial \\dy_k \\over \\partial \\theta_i} \\cinvkl {\\partial \\dy_l \\over \\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "As before, \n",
    "$$\n",
    "\\sigma_{ij} = F_{ij}^{-1}\n",
    "$$\n",
    "is the expected covariance matrix of the parameter errors, with $(F_{ii}^{-1})^{1/2}$ the expected error on parameter $\\theta_i$ if all parameters must be estimated from the data.\n",
    "\n",
    "This is equivalent to the result on pp. 3 and 4 of Gould (2003), with the notational translations\n",
    "$$\n",
    "{\\cal B}_{kl} = \\cinvkl \\qquad b_{ij} = F_{ij} \\qquad c_{ij}=F_{ij}^{-1}.\n",
    "$$\n",
    "\n",
    "\n",
    "Importantly, Gould also derives the solution for the minimum $\\chi^2$ (maximum likelihood) values of the parameters by requiring $\\partial\\chi^2/\\partial\\theta_i = 0$.\n",
    "\n",
    "The result is\n",
    "$$\n",
    "\\hat{\\theta_i} = F_{ij}^{-1}\\left[y_k\\cinvkl f_j(x_l)\\right] \\quad \n",
    "  (= c_{ij} d_{j} \\hbox{ in Gould's notation}),\n",
    "$$\n",
    "where there is an implicit sum over $k,l$ inside the $[..]$, and a sum over $j$. See his page 3, and note a_i = c_{ij} d_j$. \n",
    "\n",
    "_This is a general result for $\\chi^2$ fitting of a model that is\n",
    "linear in the parameters $\\theta_i$,\n",
    "with $F_{ij}$ defined as $f_i(x_k)\\cinvkl f_j(x_l)$._\n",
    "\n",
    "\n",
    "For a diagonal covariance matrix $\\cinvkl = \\sigma_{k}^{-2}\\delta_{kl}$, the Fisher matrix is \n",
    "$$\n",
    "F_{ij} = \\sum_{k=1}^n {f_i(x_k) f_j(x_k) \\over \\sigma_k^2}\n",
    "$$ \n",
    "and \n",
    "$$\n",
    "y_k \\cinvkl f_j(x_l) = \\sum_{k=1}^n {y_k f_j(x_k) \\over \\sigma_k^2}.\n",
    "$$\n",
    "\n",
    "These are the two terms to compute the parameter values for a diagonal covariance matrix. \n",
    "\n",
    "As previously emphasized, a diagonal covariance matrix does _not_ imply a diagonal Fisher matrix.  One can have independent data points but still have correlated parameter errors, and vice versa. (Recall the example of the slope and intercept of a line.)\n",
    "\n",
    "_For $\\chi^2$-minimization of a general linear model, one can find best-fit parameter values and the covariance matrix of parameter errors \"analytically\" (numerical matrix inversions may be required)._\n",
    "\n",
    "Gould (2003) also gives solutions for cases where one imposes\n",
    "constraints on the parameters (e.g., $\\theta_1 + 2\\theta_2 = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration for a straight line\n",
    "\n",
    "If we adopt a diagonal covariance matrix and further specify a straight-line model, $f_1(x)=1$, $f_2(x)=x$, we obtain our earlier result for the Fisher matrix, but I will now adopt notation from Numerical Recipes section 15.2:\n",
    "$$\n",
    "F_{ij} = \\pmatrix{S & S_x \\cr S_x & S_{xx}} \n",
    "$$\n",
    "with the inverse-variance weighted sums\n",
    "$$\n",
    "S \\equiv \\sum \\sigma_k^{-2}, \\quad S_x \\equiv \\sum x_k\\sigma_k^{-2},\n",
    "\\quad S_{xx} = \\sum x_k^2\\sigma_k^{-2}.\n",
    "$$\n",
    "The inverse Fisher matrix is\n",
    "$$\n",
    "F_{ij}^{-1} = {1\\over S\\,S_{xx} - S_x^2}\\pmatrix{S_{xx} & -S_x \\cr -S_x & S}.\n",
    "$$\n",
    "The vector $d_j \\equiv y_k\\cinvkl f_j(x_l)$ in Gould's notation is $(d_1,d_2)$ with\n",
    "$$\n",
    "d_1 = \\sum y_k\\sigma_k^{-2} \\equiv S_y, \\quad\n",
    "d_2 = \\sum x_k y_k\\sigma_k^{-2} \\equiv S_{xy}. \n",
    "$$\n",
    "\n",
    "The minimum-$\\chi^2$ solution is then\n",
    "$$\\eqalign{\n",
    "\\theta_1 &= F^{-1}_{11} d_1 + F^{-1}_{12} d_2 = \n",
    "  {S_{xx} S_y - S_x S_{xy} \\over S S_{xx} - S_x^2} \\cr\n",
    "\\theta_2 &= F^{-1}_{21} d_1 + F^{-1}_{22} d_2 = \n",
    "  {S S_{xy} - S_x S_y \\over S S_{xx} - S_x^2},\n",
    "}\n",
    "$$\n",
    "in agreement with equation 15.2.6 of NR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding a non-linear model\n",
    "\n",
    "Suppose that our model is a _non-linear_ function of our\n",
    "parameters, but we know that the correct parameters are \n",
    "small perturbations about a fiducial model with parameters\n",
    "$\\theta_{i,{\\rm fid}}$.  \n",
    "\n",
    "In this case, we can make a Taylor expansion\n",
    "$$\n",
    "\\ymod (x_k) = y_{\\rm mod,fid}(x_k) + \\Delta\\theta_i \n",
    "             {\\partial \\ymod (x_k) \\over \\partial\\theta_i},\n",
    "$$\n",
    "where $\\Delta\\theta_i = \\theta_i - \\theta_{i,{\\rm fid}}$\n",
    "and the derivative is evaluated for the fiducial values\n",
    "of all parameters.\n",
    "\n",
    "This is now a linear model with parameters $\\Delta\\theta_i$ \n",
    "instead of $\\theta_i$ and\n",
    "$$\n",
    "f_i(x_k) = {\\partial \\ymod (x_k)\\over \\partial\\theta_i}.\n",
    "$$\n",
    "\n",
    "We can use this expansion to fit parameter values\n",
    "or compute parameter errors or forecast errors\n",
    "provided that the errors are small enough that the \n",
    "linear Taylor expansion remains accurate.\n",
    "\n",
    "By definition, this expansion holds exactly for a true linear model.\n",
    "\n",
    "Most Fisher matrix forecasts implicitly assume this kind of linear \n",
    "expansion around a fiducial model, \n",
    "so they will give accurate forecasts of parameter \n",
    "errors only to the extent that the linear expansion is accurate\n",
    "over the range of parameters allowed by the data.\n",
    "\n",
    "An MCMC forecast does not rely on this linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Fisher matrices\n",
    "\n",
    "Suppose we have two data sets that are statistically independent.\n",
    "\n",
    "In this case, the joint likelihood (or posterior probability) is just the product of the individual likelihoods (or posterior probabilities), since $p(x,y)=p(x)p(y)$ for independent variables. For example, say we have likelihoods $L_1 = \\Pi p_{i,1}$ and $L_2 = \\Pi p_{k,2}$: \n",
    "\n",
    "$$\n",
    "L_{T} = L_1 + L_2 = \\Pi p_{i,1} + \\Pi p_{k, 2}. \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln L_{T} = \\Sigma p_{i,1} + \\Sigma p_{k,2}\n",
    "$$\n",
    "\n",
    "Therefore, one obtains $\\langle \\ln L \\rangle$ for the two data sets by adding the two individual values of $\\langle \\ln L \\rangle$, and the Fisher matrix for the two data sets is just the sum of the Fisher matrices for the individual data sets.\n",
    "\n",
    "$$\n",
    "F_{ij,T} = - \\left\\langle{\\partial^2\\ln L_1 \\over \\partial\\theta_i\\partial\\theta_j}\n",
    "                \\right\\rangle - \\left\\langle{\\partial^2\\ln L_2 \\over \\partial\\theta_i\\partial\\theta_j}\n",
    "                \\right\\rangle\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{ij,T} = F_{ij,1} + F_{ij, 2}\n",
    "$$\n",
    "\n",
    "\n",
    "This still holds even if the data sets are quite different in \n",
    "character provided they constrain the same underlying parameters.\n",
    "\n",
    "For example, one can forecast cosmological parameter errors that\n",
    "will be obtained by joint fits to CMB data, supernova data, and\n",
    "a direct measurement of $H_0$ by adding the Fisher matrices\n",
    "for the three data sets.\n",
    "\n",
    "This is a powerful technique.\n",
    "\n",
    "Note that Fisher information scales like an inverse variance,\n",
    "$F \\propto \\sigma^{-2}$, and information from independent\n",
    "data sets adds linearly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
